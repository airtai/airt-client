{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: This module encapsulates a CLI interface for all the methods available\n",
    "  in DataBlob class.\n",
    "output-file: cli_datablob.html\n",
    "title: CLI_DataBlob\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _cli.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt._testing.activate_by_import: Testing environment activated.\n"
     ]
    }
   ],
   "source": [
    "from airt._testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import typer\n",
    "from tabulate import tabulate\n",
    "from typer import echo\n",
    "\n",
    "from airt._cli import helper\n",
    "from airt._constant import CLIENT_DB_PASSWORD, CLIENT_DB_USERNAME\n",
    "from airt._logger import get_logger, set_level\n",
    "from airt.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import shutil\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote_plus as urlquote\n",
    "\n",
    "import pytest\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "from sqlmodel import create_engine\n",
    "from typer.testing import CliRunner\n",
    "\n",
    "import airt._sanitizer\n",
    "from airt._constant import (\n",
    "    CLIENT_DB_PASSWORD,\n",
    "    CLIENT_DB_USERNAME,\n",
    "    CLIENT_NAME,\n",
    "    SERVER_URL,\n",
    "    SERVICE_PASSWORD,\n",
    "    SERVICE_TOKEN,\n",
    "    SERVICE_USERNAME,\n",
    ")\n",
    "from airt.client import DataBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "app = typer.Typer(\n",
    "    help=\"\"\"A set of commands for importing and processing data from sources such as CSV/parquet files, databases, AWS S3 buckets, and Azure Blob Storage.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = CliRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_level(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] __main__: This is a warning\n",
      "[ERROR] __main__: This is an error\n"
     ]
    }
   ],
   "source": [
    "# Testing logger settings\n",
    "\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.WARNING\n",
    "\n",
    "logger.debug(\"This is a debug message\")\n",
    "logger.info(\"This is an info\")\n",
    "logger.warning(\"This is a warning\")\n",
    "logger.error(\"This is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper context manager for testing\n",
    "\n",
    "_airt_service_token = None\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def set_airt_service_token_envvar():\n",
    "    global _airt_service_token\n",
    "    if _airt_service_token is None:\n",
    "        display(\"_airt_service_token is None, getting a token...\")\n",
    "\n",
    "        username = os.environ[SERVICE_USERNAME]\n",
    "        password = os.environ[SERVICE_PASSWORD]\n",
    "\n",
    "        Client.get_token(username=username, password=password)\n",
    "        _airt_service_token = Client.auth_token\n",
    "\n",
    "    try:\n",
    "        os.environ[SERVICE_TOKEN] = _airt_service_token\n",
    "\n",
    "        yield\n",
    "    finally:\n",
    "        del os.environ[SERVICE_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_airt_service_token is None, getting a token...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'*******************************************************************************************************************************'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with set_airt_service_token_envvar():\n",
    "    display(\"*\" * len((os.environ[SERVICE_TOKEN])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_has_help(xs: List[str]):\n",
    "    result = runner.invoke(app, xs + [\"--help\"])\n",
    "\n",
    "    display(result.stdout)\n",
    "    assert \" \".join(xs) in result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_S3_URI = \"s3://test-airt-service/ecommerce_behavior_notebooks\"\n",
    "TEST_S3_CSV_URI = \"s3://test-airt-service/ecommerce_behavior_csv\"\n",
    "TEST_AZURE_URI = \"https://testairtservice.blob.core.windows.net/test-container/ecommerce_behavior_notebooks\"\n",
    "RANDOM_UUID_FOR_TESTING = \"00000000-0000-0000-0000-000000000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hypens_from_id(id: str) -> str:\n",
    "    return \"\".join((id).split(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00000000000000000000000000000000'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = remove_hypens_from_id(RANDOM_UUID_FOR_TESTING)\n",
    "assert len(actual) == 32\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create a datablob\n",
    "\n",
    "\n",
    "_db = None\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def generate_db(force_create: bool = False):\n",
    "    global _db\n",
    "\n",
    "    if _db is None or force_create:\n",
    "        _db = DataBlob.from_s3(\n",
    "            uri=TEST_S3_URI,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        )\n",
    "\n",
    "        display(f\"{_db.uuid=}\")\n",
    "        assert len(remove_hypens_from_id(_db.uuid)) == 32\n",
    "\n",
    "        _db.progress_bar()\n",
    "\n",
    "    yield _db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command()\n",
    "@helper.display_formated_table\n",
    "@helper.requires_auth_token\n",
    "def details(\n",
    "    uuid: str = typer.Argument(\n",
    "        ...,\n",
    "        help=\"Datablob uuid.\",\n",
    "    ),\n",
    "    format: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--format\",\n",
    "        \"-f\",\n",
    "        help=\"Format output and show only the given column(s) values.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    ") -> Dict[\"str\", Union[pd.DataFrame, str]]:\n",
    "    \"\"\"Return details of a datablob.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob(uuid=uuid)\n",
    "    df = db.details()\n",
    "\n",
    "    df[\"pulled_on\"] = helper.humanize_date(df[\"pulled_on\"])\n",
    "    df[\"folder_size\"] = helper.humanize_size(df[\"folder_size\"])\n",
    "\n",
    "    return {\"df\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: details [OPTIONS] UUID\\n\\n  Return details of a datablob.\\n\\nArguments:\\n  UUID  Datablob uuid.  [required]\\n\\nOptions:\\n  -f, --format TEXT     Format output and show only the given column(s) values.\\n  -d, --debug           Set logger level to DEBUG and output everything.\\n  --install-completion  Install completion for the current shell.\\n  --show-completion     Show completion for the current shell, to copy it or\\n                        customize the installation.\\n  --help                Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | include: false\n",
    "\n",
    "assert_has_help([\"details\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_db.uuid='823c12cd-1c46-4136-96f3-d22841a447c9'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'823c12cd-1c46-4136-96f3-d22841a447c9\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for details\n",
    "# Testing positive scenario\n",
    "\n",
    "# Helper function to extract ID\n",
    "\n",
    "\n",
    "def extract_id(res) -> str:\n",
    "    r = (res.split(\"\\n\")[1]).strip()\n",
    "    return r.split(\" \")[0]\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    with generate_db() as db:\n",
    "        db_uuid = db.uuid\n",
    "\n",
    "        # Getting Details of the data source\n",
    "        format_str = \"{'datablob_uuid': '{}'}\"\n",
    "        result = runner.invoke(app, [db_uuid, \"--format\", format_str])\n",
    "\n",
    "        display(result.stdout)\n",
    "\n",
    "        assert result.exit_code == 0\n",
    "        assert result.stdout == f\"{db_uuid}\\n\", f\"{result.stdout=} {db_uuid=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: The datablob uuid is incorrect. Please try again.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | include: false\n",
    "\n",
    "# Tests for details\n",
    "# Testing negative scenario. Passing invalid data_id\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    data_uuid = RANDOM_UUID_FOR_TESTING\n",
    "    result = runner.invoke(app, [data_uuid])\n",
    "\n",
    "    display(result.stdout)\n",
    "\n",
    "    assert result.exit_code == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command(\"from-s3\")\n",
    "@helper.requires_auth_token\n",
    "def from_s3(\n",
    "    uri: str = typer.Argument(..., help=\"The AWS S3 bucket uri.\"),\n",
    "    access_key: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        help=\"Access key for the S3 bucket. If **None** (default value), then the value from **AWS_ACCESS_KEY_ID** environment variable is used.\",\n",
    "    ),\n",
    "    secret_key: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        help=\"Secret key for the S3 bucket. If **None** (default value), then the value from **AWS_SECRET_ACCESS_KEY** environment variable is used.\",\n",
    "    ),\n",
    "    cloud_provider: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--cloud-provider\",\n",
    "        \"-cp\",\n",
    "        help=\"The destination cloud storage provider's name to store the datablob. Currently, the API only supports **aws** and **azure** as cloud storage providers. If **None** (default value), then **aws**  will be used as the cloud storage provider.\",\n",
    "    ),\n",
    "    region: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--region\",\n",
    "        \"-r\",\n",
    "        help=\"The destination cloud provider's region to save your datablob. If **None** (default value) then the default region will be assigned based on the cloud provider. \"\n",
    "        \"In the case of **aws**, the datablob's source bucket region will be used and in the case of **azure**, **westeurope** will be used. \"\n",
    "        \"The supported AWS regions are: ap-northeast-1, ap-northeast-2, ap-south-1, ap-southeast-1, ap-southeast-2, ca-central-1, eu-central-1, \"\n",
    "        \"eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2. The supported Azure Blob Storage \"\n",
    "        \"regions are: australiacentral, australiacentral2, australiaeast, australiasoutheast, brazilsouth, canadacentral, canadaeast, centralindia, \"\n",
    "        \"centralus, eastasia, eastus, eastus2, francecentral, francesouth, germanynorth, germanywestcentral, japaneast, japanwest, koreacentral, koreasouth, \"\n",
    "        \"northcentralus, northeurope, norwayeast, norwaywest, southafricanorth, southafricawest, southcentralus, southeastasia, southindia, switzerlandnorth, \"\n",
    "        \"switzerlandwest, uaecentral, uaenorth, uksouth, ukwest, westcentralus, westeurope, westindia, westus, westus2.\",\n",
    "    ),\n",
    "    tag: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--tag\",\n",
    "        \"-t\",\n",
    "        help=\"A string to tag the datablob. If not passed, then the tag **latest** will be assigned to the datablob.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output datablob uuid only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"Create and return a datablob that encapsulates the data from an AWS S3 bucket.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob.from_s3(\n",
    "        uri=uri,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        cloud_provider=cloud_provider,\n",
    "        region=region,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "    if quiet:\n",
    "        db.wait()\n",
    "\n",
    "        typer.echo(f\"{db.uuid}\")\n",
    "    else:\n",
    "        typer.echo(f\"Pulling datablob uuid: {db.uuid}\")\n",
    "\n",
    "        db.progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Usage: root from-s3 [OPTIONS] URI\\n\\n  Create and return a datablob that encapsulates the data from an AWS S3 bucket.\\n\\nArguments:\\n  URI  The AWS S3 bucket uri.  [required]\\n\\nOptions:\\n  --access-key TEXT           Access key for the S3 bucket. If **None** (default\\n                              value), then the value from **AWS_ACCESS_KEY_ID**\\n                              environment variable is used.\\n  --secret-key TEXT           Secret key for the S3 bucket. If **None** (default\\n                              value), then the value from\\n                              **AWS_SECRET_ACCESS_KEY** environment variable is\\n                              used.\\n  -cp, --cloud-provider TEXT  The destination cloud storage provider's name to\\n                              store the datablob. Currently, the API only\\n                              supports **aws** and **azure** as cloud storage\\n                              providers. If **None** (default value), then\\n                              **aws**  will be used as the cloud storage\\n                              provider.\\n  -r, --region TEXT           The destination cloud provider's region to save\\n                              your datablob. If **None** (default value) then\\n                              the default region will be assigned based on the\\n                              cloud provider. In the case of **aws**, the\\n                              datablob's source bucket region will be used and\\n                              in the case of **azure**, **westeurope** will be\\n                              used. The supported AWS regions are: ap-\\n                              northeast-1, ap-northeast-2, ap-south-1, ap-\\n                              southeast-1, ap-southeast-2, ca-central-1, eu-\\n                              central-1, eu-north-1, eu-west-1, eu-west-2, eu-\\n                              west-3, sa-east-1, us-east-1, us-east-2, us-\\n                              west-1, us-west-2. The supported Azure Blob\\n                              Storage regions are: australiacentral,\\n                              australiacentral2, australiaeast,\\n                              australiasoutheast, brazilsouth, canadacentral,\\n                              canadaeast, centralindia, centralus, eastasia,\\n                              eastus, eastus2, francecentral, francesouth,\\n                              germanynorth, germanywestcentral, japaneast,\\n                              japanwest, koreacentral, koreasouth,\\n                              northcentralus, northeurope, norwayeast,\\n                              norwaywest, southafricanorth, southafricawest,\\n                              southcentralus, southeastasia, southindia,\\n                              switzerlandnorth, switzerlandwest, uaecentral,\\n                              uaenorth, uksouth, ukwest, westcentralus,\\n                              westeurope, westindia, westus, westus2.\\n  -t, --tag TEXT              A string to tag the datablob. If not passed, then\\n                              the tag **latest** will be assigned to the\\n                              datablob.\\n  -q, --quiet                 Output datablob uuid only.\\n  -d, --debug                 Set logger level to DEBUG and output everything.\\n  --help                      Show this message and exit.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | include: false\n",
    "\n",
    "assert_has_help([\"from-s3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to test multiple scenarios.\n",
    "\n",
    "\n",
    "def assert_datablob(xs: List[str]):\n",
    "    # Testing Negative scenario\n",
    "    # Creating datablob without token\n",
    "\n",
    "    # Clearing previously set env variables\n",
    "    _token_flag = False\n",
    "\n",
    "    if os.environ.get(SERVICE_TOKEN):\n",
    "        _token_flag = True\n",
    "        airt_service_token = os.environ[SERVICE_TOKEN]\n",
    "        del os.environ[SERVICE_TOKEN]\n",
    "\n",
    "    result = runner.invoke(app, xs)\n",
    "    display(result.stdout)\n",
    "    assert result.exit_code == 1\n",
    "    assert (\n",
    "        f\"KeyError: The environment variable '{SERVICE_TOKEN}' is not set.\\n\\nPlease run the command '{CLIENT_NAME} token'\"\n",
    "        in result.stdout\n",
    "    )\n",
    "\n",
    "    if _token_flag:\n",
    "        os.environ[SERVICE_TOKEN] = airt_service_token\n",
    "\n",
    "    # Testing Positive scenario\n",
    "    # With and without quite\n",
    "\n",
    "    with set_airt_service_token_envvar():\n",
    "        # Without quiet (verbose)\n",
    "        result = runner.invoke(app, xs)\n",
    "        display(result.stdout)\n",
    "        assert \"Pulling datablob uuid:\" in result.stdout, result.stdout\n",
    "\n",
    "        # With quiet\n",
    "        display(\"*\" * 120)\n",
    "        result = runner.invoke(app, xs + [\"-q\"])\n",
    "        display(result.stdout)\n",
    "        assert len(remove_hypens_from_id(result.stdout[:-1])) == 32, len(\n",
    "            result.stdout[:-1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"KeyError: The environment variable 'AIRT_SERVICE_TOKEN' is not set.\\n\\nPlease run the command 'airt token' to get the application token and set it in the environment variable `AIRT_SERVICE_TOKEN`.\\n\\nTry 'airt token --help' for help.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Pulling datablob uuid: f48bdad3-cbdf-4afc-9d35-38c626a58a2e\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r  0%|          | 0/1 [00:10<?, ?it/s]\\r  0%|          | 0/1 [00:15<?, ?it/s]\\r  0%|          | 0/1 [00:20<?, ?it/s]\\r  0%|          | 0/1 [00:25<?, ?it/s]\\r  0%|          | 0/1 [00:30<?, ?it/s]\\r  0%|          | 0/1 [00:35<?, ?it/s]\\r100%|██████████| 1/1 [00:40<00:00,  5.06s/it]\\r100%|██████████| 1/1 [00:40<00:00, 40.54s/it]\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'************************************************************************************************************************'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'6969f7a0-8a18-49a2-af7f-8d7b9f639c08\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for Datablob s3\n",
    "\n",
    "cmd = [\"from-s3\", f\"{TEST_S3_CSV_URI}\"]\n",
    "\n",
    "assert_datablob(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command(\"from-azure-blob-storage\")\n",
    "@helper.requires_auth_token\n",
    "def from_azure_blob_storage(\n",
    "    uri: str = typer.Argument(..., help=\"Azure Blob Storage URI of the source file.\"),\n",
    "    credential: str = typer.Option(\n",
    "        ...,\n",
    "        \"--credential\",\n",
    "        \"-c\",\n",
    "        help=\"Credential to access the Azure Blob Storage.\",\n",
    "    ),\n",
    "    cloud_provider: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--cloud-provider\",\n",
    "        \"-cp\",\n",
    "        help=\"The destination cloud storage provider's name to store the datablob. Currently, the API only supports **aws** and **azure** as cloud storage providers. If **None** (default value), then **azure**  will be used as the cloud storage provider.\",\n",
    "    ),\n",
    "    region: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--region\",\n",
    "        \"-r\",\n",
    "        help=\"The destination cloud provider's region to save your datablob. If **None** (default value) then the default region will be assigned based on the cloud provider. \"\n",
    "        \"In the case of **aws**, **eu-west-1** will be used and in the case of **azure**, **westeurope** will be used. \"\n",
    "        \"The supported AWS regions are: ap-northeast-1, ap-northeast-2, ap-south-1, ap-southeast-1, ap-southeast-2, ca-central-1, eu-central-1, \"\n",
    "        \"eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2. The supported Azure Blob Storage \"\n",
    "        \"regions are: australiacentral, australiacentral2, australiaeast, australiasoutheast, brazilsouth, canadacentral, canadaeast, centralindia, \"\n",
    "        \"centralus, eastasia, eastus, eastus2, francecentral, francesouth, germanynorth, germanywestcentral, japaneast, japanwest, koreacentral, koreasouth, \"\n",
    "        \"northcentralus, northeurope, norwayeast, norwaywest, southafricanorth, southafricawest, southcentralus, southeastasia, southindia, switzerlandnorth, \"\n",
    "        \"switzerlandwest, uaecentral, uaenorth, uksouth, ukwest, westcentralus, westeurope, westindia, westus, westus2.\",\n",
    "    ),\n",
    "    tag: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--tag\",\n",
    "        \"-t\",\n",
    "        help=\"A string to tag the datablob. If not passed, then the tag **latest** will be assigned to the datablob.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output datablob uuid only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"Create and return a datablob that encapsulates the data from an Azure Blob Storage.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob.from_azure_blob_storage(\n",
    "        uri=uri,\n",
    "        credential=credential,\n",
    "        cloud_provider=cloud_provider,\n",
    "        region=region,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "    if quiet:\n",
    "        db.wait()\n",
    "\n",
    "        typer.echo(f\"{db.uuid}\")\n",
    "    else:\n",
    "        typer.echo(f\"Pulling datablob uuid: {db.uuid}\")\n",
    "\n",
    "        db.progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Usage: root from-azure-blob-storage [OPTIONS] URI\\n\\n  Create and return a datablob that encapsulates the data from an Azure Blob\\n  Storage.\\n\\nArguments:\\n  URI  Azure Blob Storage URI of the source file.  [required]\\n\\nOptions:\\n  -c, --credential TEXT       Credential to access the Azure Blob Storage.\\n                              [required]\\n  -cp, --cloud-provider TEXT  The destination cloud storage provider's name to\\n                              store the datablob. Currently, the API only\\n                              supports **aws** and **azure** as cloud storage\\n                              providers. If **None** (default value), then\\n                              **azure**  will be used as the cloud storage\\n                              provider.\\n  -r, --region TEXT           The destination cloud provider's region to save\\n                              your datablob. If **None** (default value) then\\n                              the default region will be assigned based on the\\n                              cloud provider. In the case of **aws**, **eu-\\n                              west-1** will be used and in the case of\\n                              **azure**, **westeurope** will be used. The\\n                              supported AWS regions are: ap-northeast-1, ap-\\n                              northeast-2, ap-south-1, ap-southeast-1, ap-\\n                              southeast-2, ca-central-1, eu-central-1, eu-\\n                              north-1, eu-west-1, eu-west-2, eu-west-3, sa-\\n                              east-1, us-east-1, us-east-2, us-west-1, us-\\n                              west-2. The supported Azure Blob Storage regions\\n                              are: australiacentral, australiacentral2,\\n                              australiaeast, australiasoutheast, brazilsouth,\\n                              canadacentral, canadaeast, centralindia,\\n                              centralus, eastasia, eastus, eastus2,\\n                              francecentral, francesouth, germanynorth,\\n                              germanywestcentral, japaneast, japanwest,\\n                              koreacentral, koreasouth, northcentralus,\\n                              northeurope, norwayeast, norwaywest,\\n                              southafricanorth, southafricawest, southcentralus,\\n                              southeastasia, southindia, switzerlandnorth,\\n                              switzerlandwest, uaecentral, uaenorth, uksouth,\\n                              ukwest, westcentralus, westeurope, westindia,\\n                              westus, westus2.\\n  -t, --tag TEXT              A string to tag the datablob. If not passed, then\\n                              the tag **latest** will be assigned to the\\n                              datablob.\\n  -q, --quiet                 Output datablob uuid only.\\n  -d, --debug                 Set logger level to DEBUG and output everything.\\n  --help                      Show this message and exit.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"from-azure-blob-storage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"KeyError: The environment variable 'AIRT_SERVICE_TOKEN' is not set.\\n\\nPlease run the command 'airt token' to get the application token and set it in the environment variable `AIRT_SERVICE_TOKEN`.\\n\\nTry 'airt token --help' for help.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Pulling datablob uuid: 82e15303-adcd-4f56-9347-0fb3dbf8b9d2\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r  0%|          | 0/1 [00:10<?, ?it/s]\\r  0%|          | 0/1 [00:15<?, ?it/s]\\r  0%|          | 0/1 [00:20<?, ?it/s]\\r  0%|          | 0/1 [00:25<?, ?it/s]\\r  0%|          | 0/1 [00:30<?, ?it/s]\\r100%|██████████| 1/1 [00:35<00:00,  5.06s/it]\\r100%|██████████| 1/1 [00:35<00:00, 35.50s/it]\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'************************************************************************************************************************'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'eb7d485d-6a9b-499a-9157-262077c1cbef\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for from-azure-blob-storage\n",
    "# Positive Scenario: Passing credential in arguments\n",
    "\n",
    "storage_client = StorageManagementClient(\n",
    "    DefaultAzureCredential(), os.environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
    ")\n",
    "keys = storage_client.storage_accounts.list_keys(\"test-airt-service\", \"testairtservice\")\n",
    "credential = keys.keys[0].value\n",
    "\n",
    "cmd = [\n",
    "    \"from-azure-blob-storage\",\n",
    "    f\"{TEST_AZURE_URI}\",\n",
    "    \"--credential\",\n",
    "    f\"{credential}\",\n",
    "]\n",
    "\n",
    "assert_datablob(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7acc7398-1d8e-4454-8ded-e355f8414c7a\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7acc7398-1d8e-4454-8ded-e355f8414c7a'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type                source                                                                                     region      cloud_provider    tags    pulled_on      folder_size    user_uuid                             error    disabled    ready\\n7acc7398-1d8e-4454-8ded-e355f8414c7a  <none>              azure_blob_storage  https://testairtservice.blob.core.windows.net/test-container/ecommerce_behavior_notebooks  westeurope  azure             latest  7 seconds ago  10.2 MB        4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       True\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7c376650-5d91-4124-9057-9dfdd6984fbb\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7c376650-5d91-4124-9057-9dfdd6984fbb'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type                source                                                                                     region       cloud_provider    tags    pulled_on       folder_size    user_uuid                             error    disabled    ready\\n7c376650-5d91-4124-9057-9dfdd6984fbb  <none>              azure_blob_storage  https://testairtservice.blob.core.windows.net/test-container/ecommerce_behavior_notebooks  northeurope  azure             latest  11 seconds ago  10.2 MB        4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       True\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for from-azure-blob-storage\n",
    "# Positive Scenario: Validating the default region\n",
    "with set_airt_service_token_envvar():\n",
    "    for region in [\"westeurope\", \"northeurope\"]:\n",
    "        cmd = [\n",
    "            \"from-azure-blob-storage\",\n",
    "            f\"{TEST_AZURE_URI}\",\n",
    "            \"--credential\",\n",
    "            f\"{credential}\",\n",
    "            \"--cloud-provider\",\n",
    "            \"azure\",\n",
    "            \"--region\",\n",
    "            f\"{region}\",\n",
    "            \"-q\",\n",
    "        ]\n",
    "\n",
    "        result = runner.invoke(app, cmd)\n",
    "        display(result.stdout)\n",
    "\n",
    "        db_uuid = result.stdout[:-1]\n",
    "        display(db_uuid)\n",
    "        assert len(remove_hypens_from_id(db_uuid)) == 32\n",
    "\n",
    "        result = runner.invoke(app, [\"details\", db_uuid])\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 0\n",
    "        assert region in result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command(\"from-mysql\")\n",
    "@helper.requires_auth_token\n",
    "def from_mysql(\n",
    "    host: str = typer.Option(..., help=\"Remote database host name.\"),\n",
    "    database: str = typer.Option(..., help=\"Database name.\"),\n",
    "    table: str = typer.Option(..., help=\"Table name.\"),\n",
    "    port: int = typer.Option(\n",
    "        3306,\n",
    "        help=\"Host port number. If not passed, then the default value **3306** will be used.\",\n",
    "    ),\n",
    "    cloud_provider: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--cloud-provider\",\n",
    "        \"-cp\",\n",
    "        help=\"The destination cloud storage provider's name to store the datablob. Currently, the API only supports **aws** and **azure** as cloud storage providers. If **None** (default value), then **aws**  will be used as the cloud storage provider.\",\n",
    "    ),\n",
    "    region: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--region\",\n",
    "        \"-r\",\n",
    "        help=\"The destination cloud provider's region to save your datablob. If **None** (default value) then the default region will be assigned based on the cloud provider. \"\n",
    "        \"In the case of **aws**, **eu-west-1** will be used and in the case of **azure**, **westeurope** will be used. \"\n",
    "        \"The supported AWS regions are: ap-northeast-1, ap-northeast-2, ap-south-1, ap-southeast-1, ap-southeast-2, ca-central-1, eu-central-1, \"\n",
    "        \"eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2. The supported Azure Blob Storage \"\n",
    "        \"regions are: australiacentral, australiacentral2, australiaeast, australiasoutheast, brazilsouth, canadacentral, canadaeast, centralindia, \"\n",
    "        \"centralus, eastasia, eastus, eastus2, francecentral, francesouth, germanynorth, germanywestcentral, japaneast, japanwest, koreacentral, koreasouth, \"\n",
    "        \"northcentralus, northeurope, norwayeast, norwaywest, southafricanorth, southafricawest, southcentralus, southeastasia, southindia, switzerlandnorth, \"\n",
    "        \"switzerlandwest, uaecentral, uaenorth, uksouth, ukwest, westcentralus, westeurope, westindia, westus, westus2.\",\n",
    "    ),\n",
    "    username: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--username\",\n",
    "        \"-u\",\n",
    "        help=f'Database username. If not passed, the default value \"root\" will be used unless the value is explicitly set in the environment variable **{CLIENT_DB_USERNAME}**.',\n",
    "    ),\n",
    "    password: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--password\",\n",
    "        \"-p\",\n",
    "        help=f'Database password. If not passed, the default value \"\" will be used unless the value is explicitly set in the environment variable **{CLIENT_DB_PASSWORD}**.',\n",
    "    ),\n",
    "    tag: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--tag\",\n",
    "        \"-t\",\n",
    "        help=\"A string to tag the datablob. If not passed, then the tag **latest** will be assigned to the datablob.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output datablob uuid only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"Create and return a datablob that encapsulates the data from a mysql database.\n",
    "\n",
    "    If the database requires authentication, pass the username/password as commandline arguments or store it in\n",
    "    the **AIRT_CLIENT_DB_USERNAME** and **AIRT_CLIENT_DB_PASSWORD** environment variables.\n",
    "    \"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob.from_mysql(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        port=port,\n",
    "        table=table,\n",
    "        username=username,\n",
    "        password=password,\n",
    "        cloud_provider=cloud_provider,\n",
    "        region=region,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "    if quiet:\n",
    "        db.wait()\n",
    "        typer.echo(f\"{db.uuid}\")\n",
    "    else:\n",
    "        typer.echo(f\"Pulling datablob uuid: {db.uuid}\")\n",
    "        db.progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: root from-mysql [OPTIONS]\\n\\n  Create and return a datablob that encapsulates the data from a mysql database.\\n\\n  If the database requires authentication, pass the username/password as\\n  commandline arguments or store it in the **AIRT_CLIENT_DB_USERNAME** and\\n  **AIRT_CLIENT_DB_PASSWORD** environment variables.\\n\\nOptions:\\n  --host TEXT                 Remote database host name.  [required]\\n  --database TEXT             Database name.  [required]\\n  --table TEXT                Table name.  [required]\\n  --port INTEGER              Host port number. If not passed, then the default\\n                              value **3306** will be used.  [default: 3306]\\n  -cp, --cloud-provider TEXT  The destination cloud storage provider\\'s name to\\n                              store the datablob. Currently, the API only\\n                              supports **aws** and **azure** as cloud storage\\n                              providers. If **None** (default value), then\\n                              **aws**  will be used as the cloud storage\\n                              provider.\\n  -r, --region TEXT           The destination cloud provider\\'s region to save\\n                              your datablob. If **None** (default value) then\\n                              the default region will be assigned based on the\\n                              cloud provider. In the case of **aws**, **eu-\\n                              west-1** will be used and in the case of\\n                              **azure**, **westeurope** will be used. The\\n                              supported AWS regions are: ap-northeast-1, ap-\\n                              northeast-2, ap-south-1, ap-southeast-1, ap-\\n                              southeast-2, ca-central-1, eu-central-1, eu-\\n                              north-1, eu-west-1, eu-west-2, eu-west-3, sa-\\n                              east-1, us-east-1, us-east-2, us-west-1, us-\\n                              west-2. The supported Azure Blob Storage regions\\n                              are: australiacentral, australiacentral2,\\n                              australiaeast, australiasoutheast, brazilsouth,\\n                              canadacentral, canadaeast, centralindia,\\n                              centralus, eastasia, eastus, eastus2,\\n                              francecentral, francesouth, germanynorth,\\n                              germanywestcentral, japaneast, japanwest,\\n                              koreacentral, koreasouth, northcentralus,\\n                              northeurope, norwayeast, norwaywest,\\n                              southafricanorth, southafricawest, southcentralus,\\n                              southeastasia, southindia, switzerlandnorth,\\n                              switzerlandwest, uaecentral, uaenorth, uksouth,\\n                              ukwest, westcentralus, westeurope, westindia,\\n                              westus, westus2.\\n  -u, --username TEXT         Database username. If not passed, the default\\n                              value \"root\" will be used unless the value is\\n                              explicitly set in the environment variable\\n                              **AIRT_CLIENT_DB_USERNAME**.\\n  -p, --password TEXT         Database password. If not passed, the default\\n                              value \"\" will be used unless the value is\\n                              explicitly set in the environment variable\\n                              **AIRT_CLIENT_DB_PASSWORD**.\\n  -t, --tag TEXT              A string to tag the datablob. If not passed, then\\n                              the tag **latest** will be assigned to the\\n                              datablob.\\n  -q, --quiet                 Output datablob uuid only.\\n  -d, --debug                 Set logger level to DEBUG and output everything.\\n  --help                      Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"from-mysql\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pulling datablob uuid: 1a76ae7a-7668-4def-b172-0d50484f2530\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\nError: (MySQLdb.OperationalError) (2005, \"Unknown MySQL server host \\'db.staging.airt.ai\\' (-2)\")\\n(Background on this error at: https://sqlalche.me/e/14/e3q8)\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests for db. Testing negative scenario.\n",
    "# Passing invalid host address\n",
    "\n",
    "cmd = [\n",
    "    \"from-mysql\",\n",
    "    \"--host\",\n",
    "    \"db.staging.airt.ai\",\n",
    "    \"--database\",\n",
    "    \"test\",\n",
    "    \"--table\",\n",
    "    \"test\",\n",
    "]\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    # Without quiet (verbose)\n",
    "    result = runner.invoke(app, cmd)\n",
    "    display(result.stdout)\n",
    "    assert \"Unknown MySQL server host 'db.staging.airt.ai'\" in result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/_metadata to ../../../tmp/test_s3_download_s_7mrvfp/_metadata\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.16.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.16.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.17.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.17.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.18.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.18.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.19.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.19.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.2.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.2.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.4.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.4.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.3.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.3.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/_common_metadata to ../../../tmp/test_s3_download_s_7mrvfp/_common_metadata\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.5.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.5.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.10.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.10.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.9.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.9.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.13.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.13.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.12.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.12.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.8.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.8.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.15.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.15.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.7.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.7.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.0.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.0.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.1.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.1.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.11.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.11.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.14.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.14.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.6.parquet to ../../../tmp/test_s3_download_s_7mrvfp/part.6.parquet\n",
      "_common_metadata  part.12.parquet  part.18.parquet  part.6.parquet\n",
      "_metadata\t  part.13.parquet  part.19.parquet  part.7.parquet\n",
      "part.0.parquet\t  part.14.parquet  part.2.parquet   part.8.parquet\n",
      "part.1.parquet\t  part.15.parquet  part.3.parquet   part.9.parquet\n",
      "part.10.parquet   part.16.parquet  part.4.parquet\n",
      "part.11.parquet   part.17.parquet  part.5.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pulling datablob uuid: d4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r100%|██████████| 1/1 [00:10<00:00,  5.05s/it]\\r100%|██████████| 1/1 [00:10<00:00, 10.15s/it]\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'79652c84-fdaf-445f-a9cd-95e4901e0eb4'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type    source                                               region     cloud_provider    tags    pulled_on      folder_size    user_uuid                             error    disabled    ready\\n79652c84-fdaf-445f-a9cd-95e4901e0eb4  <none>              db      mysql://harish-mysql:3306/airt_service/test_db_pull  eu-west-1  aws               latest  3 seconds ago  8.0 MB         4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       True\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Pulling datablob uuid: 155a1e9c-2e52-4639-ad26-b462dc0315c4\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r  0%|          | 0/1 [00:10<?, ?it/s]\\r100%|██████████| 1/1 [00:15<00:00,  5.06s/it]\\r100%|██████████| 1/1 [00:15<00:00, 15.50s/it]\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'868229e8-0f95-4eb5-8db6-30e329654a0d'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type    source                                               region     cloud_provider    tags    pulled_on      folder_size    user_uuid                             error    disabled    ready\\n868229e8-0f95-4eb5-8db6-30e329654a0d  <none>              db      mysql://harish-mysql:3306/airt_service/test_db_pull  eu-west-3  aws               latest  3 seconds ago  8.0 MB         4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       True\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests for db. Testing positive scenario.\n",
    "\n",
    "# Helper function to create new table in the mysql db\n",
    "\n",
    "\n",
    "def get_db_engine():\n",
    "    username = os.environ[\"DB_USERNAME\"]\n",
    "    password = os.environ[\"DB_PASSWORD\"]\n",
    "    host = os.environ[\"DB_HOST\"]\n",
    "    port = int(os.environ[\"DB_PORT\"])\n",
    "    database = os.environ[\"DB_DATABASE\"]\n",
    "    database_server = os.environ[\"DB_DATABASE_SERVER\"]\n",
    "\n",
    "    quoted_password = urlquote(password)\n",
    "    conn_str = (\n",
    "        f\"{database_server}://{username}:{quoted_password}@{host}:{port}/{database}\"\n",
    "    )\n",
    "    engine = create_engine(conn_str)\n",
    "\n",
    "    return engine\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory(prefix=\"test_s3_download_\") as d:\n",
    "    !aws s3 sync {TEST_S3_URI} {d}\n",
    "    !ls {d}\n",
    "\n",
    "    engine = get_db_engine()\n",
    "\n",
    "    df = pd.read_parquet(d)\n",
    "    try:\n",
    "        df.to_sql(\"test_db_pull\", con=engine, if_exists=\"fail\")\n",
    "    except ValueError as e:\n",
    "        display(e)\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    for region in [\"eu-west-1\", \"eu-west-3\"]:\n",
    "        # Creating a new datasource\n",
    "        cmd = [\n",
    "            \"from-mysql\",\n",
    "            \"--host\",\n",
    "            os.environ[\"DB_HOST\"],\n",
    "            \"--database\",\n",
    "            os.environ[\"DB_DATABASE\"],\n",
    "            \"--table\",\n",
    "            \"test_db_pull\",\n",
    "            \"--username\",\n",
    "            os.environ[\"DB_USERNAME\"],\n",
    "            \"--password\",\n",
    "            os.environ[\"DB_PASSWORD\"],\n",
    "            \"--tag\",\n",
    "            \"v1.1.0\",\n",
    "        ]\n",
    "\n",
    "        cmd_q = [\n",
    "            \"from-mysql\",\n",
    "            \"--host\",\n",
    "            os.environ[\"DB_HOST\"],\n",
    "            \"--database\",\n",
    "            os.environ[\"DB_DATABASE\"],\n",
    "            \"--table\",\n",
    "            \"test_db_pull\",\n",
    "            \"--username\",\n",
    "            os.environ[\"DB_USERNAME\"],\n",
    "            \"--password\",\n",
    "            os.environ[\"DB_PASSWORD\"],\n",
    "            \"-cp\",\n",
    "            \"aws\",\n",
    "            \"--region\",\n",
    "            region,\n",
    "            \"-q\",\n",
    "        ]\n",
    "\n",
    "        # Without quiet\n",
    "        result = runner.invoke(app, cmd)\n",
    "\n",
    "        display(result.stdout)\n",
    "        assert \"Pulling datablob uuid:\" in str(result.stdout)\n",
    "\n",
    "        # With quiet\n",
    "        result = runner.invoke(app, cmd_q)\n",
    "        db_uuid = result.stdout[:-1]\n",
    "        display(db_uuid)\n",
    "        assert len(remove_hypens_from_id(db_uuid)) == 32\n",
    "\n",
    "        result = runner.invoke(app, [\"details\", db_uuid])\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 0\n",
    "        assert region in result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command(\"to-datasource\")\n",
    "@helper.requires_auth_token\n",
    "def to_datasource(\n",
    "    uuid: str = typer.Option(\n",
    "        ...,\n",
    "        help=\"Datablob uuid.\",\n",
    "    ),\n",
    "    file_type: str = typer.Option(\n",
    "        ...,\n",
    "        help='The file type of the datablob. Currently, the API only supports \"csv\" and \"parquet\" as file types.',\n",
    "    ),\n",
    "    index_column: str = typer.Option(\n",
    "        ...,\n",
    "        help=\"The column to use as index (row labels).\",\n",
    "    ),\n",
    "    sort_by: str = typer.Option(\n",
    "        ...,\n",
    "        help=\"The column(s) to sort the data. Can either be a string or a JSON encoded list of strings.\",\n",
    "    ),\n",
    "    deduplicate_data: bool = typer.Option(\n",
    "        False,\n",
    "        help=\"If set to **True** (default value **False**), the datasource will be created with duplicate rows removed.\",\n",
    "    ),\n",
    "    blocksize: str = typer.Option(\n",
    "        \"256MB\",\n",
    "        help=\"The number of bytes used to split larger files. If None, then the default value **256MB** will be used.\",\n",
    "    ),\n",
    "    kwargs_json: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        help=\"Additional JSON encoded dict arguments to use while processing the data.e.g: To skip 100 lines from the bottom of file, pass '{\"\n",
    "        '\"skipfooter\"'\n",
    "        \": 100}'\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output datasource uuid only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"Process the datablob and return a datasource object.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    kwargs = json.loads(kwargs_json) if kwargs_json else {}\n",
    "\n",
    "    try:\n",
    "        sort_by = json.loads(sort_by)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        pass\n",
    "\n",
    "    db = DataBlob(uuid=uuid)\n",
    "    ds = db.to_datasource(\n",
    "        file_type=file_type,\n",
    "        index_column=index_column,\n",
    "        sort_by=sort_by,\n",
    "        deduplicate_data=deduplicate_data,\n",
    "        blocksize=blocksize,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if quiet:\n",
    "        ds.wait()\n",
    "        typer.echo(f\"{ds.uuid}\")\n",
    "    else:\n",
    "        typer.echo(f\"Processing and pulling the datasource uuid: {ds.uuid}\")\n",
    "\n",
    "        ds.progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: root to-datasource [OPTIONS]\\n\\n  Process the datablob and return a datasource object.\\n\\nOptions:\\n  --uuid TEXT                     Datablob uuid.  [required]\\n  --file-type TEXT                The file type of the datablob. Currently, the\\n                                  API only supports \"csv\" and \"parquet\" as file\\n                                  types.  [required]\\n  --index-column TEXT             The column to use as index (row labels).\\n                                  [required]\\n  --sort-by TEXT                  The column(s) to sort the data. Can either be\\n                                  a string or a JSON encoded list of strings.\\n                                  [required]\\n  --deduplicate-data / --no-deduplicate-data\\n                                  If set to **True** (default value **False**),\\n                                  the datasource will be created with duplicate\\n                                  rows removed.  [default: no-deduplicate-data]\\n  --blocksize TEXT                The number of bytes used to split larger\\n                                  files. If None, then the default value\\n                                  **256MB** will be used.  [default: 256MB]\\n  --kwargs-json TEXT              Additional JSON encoded dict arguments to use\\n                                  while processing the data.e.g: To skip 100\\n                                  lines from the bottom of file, pass\\n                                  \\'{\"skipfooter\": 100}\\'\\n  -q, --quiet                     Output datasource uuid only.\\n  -d, --debug                     Set logger level to DEBUG and output\\n                                  everything.\\n  --help                          Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"to-datasource\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Processing and pulling the datasource uuid: 53bf8fc2-31c2-4b35-b355-5dcec1d79b4d\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r  0%|          | 0/1 [00:10<?, ?it/s]\\r  0%|          | 0/1 [00:15<?, ?it/s]\\r  0%|          | 0/1 [00:20<?, ?it/s]\\r  0%|          | 0/1 [00:25<?, ?it/s]\\r100%|██████████| 1/1 [00:30<00:00,  5.06s/it]\\r100%|██████████| 1/1 [00:30<00:00, 30.43s/it]\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'eb23a3bf-bf18-452e-a8db-1df7c52798b5\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for to-datasource\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    with generate_db() as db:\n",
    "        cmd = [\n",
    "            \"to-datasource\",\n",
    "            \"--uuid\",\n",
    "            f\"{db.uuid}\",\n",
    "            \"--file-type\",\n",
    "            \"parquet\",\n",
    "            \"--index-column\",\n",
    "            \"user_id\",\n",
    "            \"--sort-by\",\n",
    "            \"event_time\",\n",
    "            \"--kwargs-json\",\n",
    "            '{\"parse_dates\": [\"event_time\"], \"skipfooter\": 100}',\n",
    "        ]\n",
    "        result = runner.invoke(app, cmd)\n",
    "\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 0, f\"{result.stdout=}, {result.exit_code=}\"\n",
    "        assert (\n",
    "            \"Processing and pulling the datasource uuid:\" in result.stdout\n",
    "        ), result.stdout\n",
    "\n",
    "        cmd = [\n",
    "            \"to-datasource\",\n",
    "            \"--uuid\",\n",
    "            f\"{db.uuid}\",\n",
    "            \"--file-type\",\n",
    "            \"parquet\",\n",
    "            \"--index-column\",\n",
    "            \"user_id\",\n",
    "            \"--sort-by\",\n",
    "            '[\"event_time\", \"category_id\"]',\n",
    "            \"--kwargs-json\",\n",
    "            '{\"parse_dates\": [\"event_time\"], \"skipfooter\": 100}',\n",
    "            \"-q\",\n",
    "        ]\n",
    "        result = runner.invoke(app, cmd)\n",
    "\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 0, f\"{result.stdout=}, {result.exit_code=}\"\n",
    "        assert len(result.stdout[:-1].replace(\"-\", \"\").replace(\"\\n\", \"\")) == 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: \"Data has no column \\'random-col\\': use any column of [\\'event_time\\', \\'event_type\\', \\'product_id\\', \\'category_id\\', \\'category_code\\', \\'brand\\', \\'price\\', \\'user_id\\', \\'user_session\\']\"\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for to-datasource. Passing wrong index and sort column names\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    with generate_db() as db:\n",
    "        cmd = [\n",
    "            \"to-datasource\",\n",
    "            \"--uuid\",\n",
    "            f\"{db.uuid}\",\n",
    "            \"--file-type\",\n",
    "            \"parquet\",\n",
    "            \"--index-column\",\n",
    "            \"random-col\",\n",
    "            \"--sort-by\",\n",
    "            \"random-col\",\n",
    "            \"-q\",\n",
    "        ]\n",
    "        result = runner.invoke(app, cmd)\n",
    "\n",
    "        display(result.stdout)\n",
    "        assert \"'random-col'\" in result.stdout, result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command()\n",
    "@helper.display_formated_table\n",
    "@helper.requires_auth_token\n",
    "def ls(\n",
    "    offset: int = typer.Option(\n",
    "        0,\n",
    "        \"--offset\",\n",
    "        \"-o\",\n",
    "        help=\"The number of datablobs to offset at the beginning. If **None**, then the default value **0** will be used.\",\n",
    "    ),\n",
    "    limit: int = typer.Option(\n",
    "        100,\n",
    "        \"--limit\",\n",
    "        \"-l\",\n",
    "        help=\"The maximum number of datablobs to return from the server. If **None**, then the default value **100** will be used.\",\n",
    "    ),\n",
    "    disabled: bool = typer.Option(\n",
    "        False,\n",
    "        \"--disabled\",\n",
    "        help=\"If set to **True**, then only the deleted datablobs will be returned.\"\n",
    "        \"Else, the default value **False** will be used to return only the list\"\n",
    "        \"of active datablobs.\",\n",
    "    ),\n",
    "    completed: bool = typer.Option(\n",
    "        False,\n",
    "        \"--completed\",\n",
    "        help=\"If set to **True**, then only the datablobs that are successfully downloaded\"\n",
    "        \"to the server will be returned. Else, the default value **False** will be used to\"\n",
    "        \"return all the datablobs.\",\n",
    "    ),\n",
    "    format: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--format\",\n",
    "        \"-f\",\n",
    "        help=\"Format output and show only the given column(s) values.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output only datablob uuids separated by space\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    ") -> Dict[\"str\", Union[pd.DataFrame, str]]:\n",
    "    \"\"\"Return the list of datablobs.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    dbx = DataBlob.ls(\n",
    "        offset=offset, limit=limit, disabled=disabled, completed=completed\n",
    "    )\n",
    "\n",
    "    df = DataBlob.as_df(dbx)\n",
    "\n",
    "    df[\"pulled_on\"] = helper.humanize_date(df[\"pulled_on\"])\n",
    "    df[\"folder_size\"] = helper.humanize_size(df[\"folder_size\"])\n",
    "\n",
    "    return {\"df\": df, \"quite_column_name\": \"datablob_uuid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: root ls [OPTIONS]\\n\\n  Return the list of datablobs.\\n\\nOptions:\\n  -o, --offset INTEGER  The number of datablobs to offset at the beginning. If\\n                        **None**, then the default value **0** will be used.\\n                        [default: 0]\\n  -l, --limit INTEGER   The maximum number of datablobs to return from the\\n                        server. If **None**, then the default value **100** will\\n                        be used.  [default: 100]\\n  --disabled            If set to **True**, then only the deleted datablobs will\\n                        be returned.Else, the default value **False** will be\\n                        used to return only the listof active datablobs.\\n  --completed           If set to **True**, then only the datablobs that are\\n                        successfully downloadedto the server will be returned.\\n                        Else, the default value **False** will be used toreturn\\n                        all the datablobs.\\n  -f, --format TEXT     Format output and show only the given column(s) values.\\n  -q, --quiet           Output only datablob uuids separated by space\\n  -d, --debug           Set logger level to DEBUG and output everything.\\n  --help                Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"ls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         type\\n8ef21b49-8e4d-402b-a64c-f70e6487eb49  s3\\nb7c1fe3f-8187-4874-bd5b-f5d8363fc2fe  s3\\n823c12cd-1c46-4136-96f3-d22841a447c9  s3\\nf48bdad3-cbdf-4afc-9d35-38c626a58a2e  s3\\n6969f7a0-8a18-49a2-af7f-8d7b9f639c08  s3\\n82e15303-adcd-4f56-9347-0fb3dbf8b9d2  azure_blob_storage\\neb7d485d-6a9b-499a-9157-262077c1cbef  azure_blob_storage\\n7acc7398-1d8e-4454-8ded-e355f8414c7a  azure_blob_storage\\n7c376650-5d91-4124-9057-9dfdd6984fbb  azure_blob_storage\\n1a76ae7a-7668-4def-b172-0d50484f2530  db\\nd4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02  db\\n79652c84-fdaf-445f-a9cd-95e4901e0eb4  db\\n155a1e9c-2e52-4639-ad26-b462dc0315c4  db\\n868229e8-0f95-4eb5-8db6-30e329654a0d  db\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'8ef21b49-8e4d-402b-a64c-f70e6487eb49\\nb7c1fe3f-8187-4874-bd5b-f5d8363fc2fe\\n823c12cd-1c46-4136-96f3-d22841a447c9\\nf48bdad3-cbdf-4afc-9d35-38c626a58a2e\\n6969f7a0-8a18-49a2-af7f-8d7b9f639c08\\n82e15303-adcd-4f56-9347-0fb3dbf8b9d2\\neb7d485d-6a9b-499a-9157-262077c1cbef\\n7acc7398-1d8e-4454-8ded-e355f8414c7a\\n7c376650-5d91-4124-9057-9dfdd6984fbb\\n1a76ae7a-7668-4def-b172-0d50484f2530\\nd4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02\\n79652c84-fdaf-445f-a9cd-95e4901e0eb4\\n155a1e9c-2e52-4639-ad26-b462dc0315c4\\n868229e8-0f95-4eb5-8db6-30e329654a0d\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"uuids=['8ef21b49-8e4d-402b-a64c-f70e6487eb49', 'b7c1fe3f-8187-4874-bd5b-f5d8363fc2fe', '823c12cd-1c46-4136-96f3-d22841a447c9', 'f48bdad3-cbdf-4afc-9d35-38c626a58a2e', '6969f7a0-8a18-49a2-af7f-8d7b9f639c08', '82e15303-adcd-4f56-9347-0fb3dbf8b9d2', 'eb7d485d-6a9b-499a-9157-262077c1cbef', '7acc7398-1d8e-4454-8ded-e355f8414c7a', '7c376650-5d91-4124-9057-9dfdd6984fbb', '1a76ae7a-7668-4def-b172-0d50484f2530', 'd4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02', '79652c84-fdaf-445f-a9cd-95e4901e0eb4', '155a1e9c-2e52-4639-ad26-b462dc0315c4', '868229e8-0f95-4eb5-8db6-30e329654a0d']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for datasource_ls\n",
    "# Testing positive scenario. Saving the token in env variable\n",
    "\n",
    "\n",
    "def get_uuids_from_result(result) -> List[int]:\n",
    "    return [uuid for uuid in result.stdout[:-1].split(\"\\n\")]\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    # Without quiet\n",
    "    format_str = \"{'datablob_uuid': '{}', 'type': '{}'}\"\n",
    "    result = runner.invoke(app, [\"ls\", \"--format\", format_str])\n",
    "    display(result.stdout)\n",
    "\n",
    "    assert \"type\" in result.stdout\n",
    "    assert result.exit_code == 0\n",
    "\n",
    "    # With quiet\n",
    "    result = runner.invoke(app, [\"ls\", \"-q\"])\n",
    "    display(result.stdout)\n",
    "\n",
    "    assert result.exit_code == 0\n",
    "    uuids = get_uuids_from_result(result)\n",
    "    display(f\"{uuids=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"uuids=['b7c1fe3f-8187-4874-bd5b-f5d8363fc2fe']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"uuids=['b7c1fe3f-8187-4874-bd5b-f5d8363fc2fe', '823c12cd-1c46-4136-96f3-d22841a447c9', 'f48bdad3-cbdf-4afc-9d35-38c626a58a2e', '6969f7a0-8a18-49a2-af7f-8d7b9f639c08', '82e15303-adcd-4f56-9347-0fb3dbf8b9d2', 'eb7d485d-6a9b-499a-9157-262077c1cbef', '7acc7398-1d8e-4454-8ded-e355f8414c7a', '7c376650-5d91-4124-9057-9dfdd6984fbb', '1a76ae7a-7668-4def-b172-0d50484f2530', 'd4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"uuids=['b7c1fe3f-8187-4874-bd5b-f5d8363fc2fe', '823c12cd-1c46-4136-96f3-d22841a447c9', 'f48bdad3-cbdf-4afc-9d35-38c626a58a2e', '6969f7a0-8a18-49a2-af7f-8d7b9f639c08', '82e15303-adcd-4f56-9347-0fb3dbf8b9d2', 'eb7d485d-6a9b-499a-9157-262077c1cbef', '7acc7398-1d8e-4454-8ded-e355f8414c7a', '7c376650-5d91-4124-9057-9dfdd6984fbb', '1a76ae7a-7668-4def-b172-0d50484f2530', 'd4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02', '79652c84-fdaf-445f-a9cd-95e4901e0eb4', '155a1e9c-2e52-4639-ad26-b462dc0315c4', '868229e8-0f95-4eb5-8db6-30e329654a0d']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for datasource_ls\n",
    "# Testing positive scenario.\n",
    "# Testing by passing different values for  limit\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    for limit in [1, 10, 1000]:\n",
    "        offset = 1\n",
    "        result = runner.invoke(app, [\"ls\", \"--offset\", offset, \"--limit\", limit, \"-q\"])\n",
    "\n",
    "        assert result.exit_code == 0\n",
    "\n",
    "        uuids = get_uuids_from_result(result)\n",
    "        display(f\"{uuids=}\")\n",
    "        assert limit >= len(uuids) >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datablob_uuid    datasource_uuids    type    source    region    cloud_provider    tags    pulled_on    folder_size    ready\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for datasource_ls\n",
    "# Testing positive scenario.\n",
    "# Testing by passing large value for offset.\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    limit = 10\n",
    "    offset = 1_000_000\n",
    "    result = runner.invoke(app, [\"ls\", \"--offset\", offset, \"--limit\", limit])\n",
    "\n",
    "    assert result.exit_code == 0\n",
    "\n",
    "    display(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command()\n",
    "@helper.display_formated_table\n",
    "@helper.requires_auth_token\n",
    "def rm(\n",
    "    uuid: str = typer.Argument(\n",
    "        ...,\n",
    "        help=\"Datablob uuid.\",\n",
    "    ),\n",
    "    format: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--format\",\n",
    "        \"-f\",\n",
    "        help=\"Format output and show only the given column(s) values.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output the deleted datablob uuid only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    ") -> Dict[\"str\", Union[pd.DataFrame, str]]:\n",
    "    \"\"\"Delete a datablob from the server.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob(uuid=uuid)\n",
    "    df = db.delete()\n",
    "\n",
    "    df[\"pulled_on\"] = helper.humanize_date(df[\"pulled_on\"])\n",
    "    df[\"folder_size\"] = helper.humanize_size(df[\"folder_size\"])\n",
    "\n",
    "    return {\"df\": df, \"quite_column_name\": \"datablob_uuid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: root rm [OPTIONS] UUID\\n\\n  Delete a datablob from the server.\\n\\nArguments:\\n  UUID  Datablob uuid.  [required]\\n\\nOptions:\\n  -f, --format TEXT  Format output and show only the given column(s) values.\\n  -q, --quiet        Output the deleted datablob uuid only.\\n  -d, --debug        Set logger level to DEBUG and output everything.\\n  --help             Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"rm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'823c12cd-1c46-4136-96f3-d22841a447c9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['8ef21b49-8e4d-402b-a64c-f70e6487eb49',\n",
       " 'b7c1fe3f-8187-4874-bd5b-f5d8363fc2fe',\n",
       " 'f48bdad3-cbdf-4afc-9d35-38c626a58a2e',\n",
       " '6969f7a0-8a18-49a2-af7f-8d7b9f639c08',\n",
       " '82e15303-adcd-4f56-9347-0fb3dbf8b9d2',\n",
       " 'eb7d485d-6a9b-499a-9157-262077c1cbef',\n",
       " '7acc7398-1d8e-4454-8ded-e355f8414c7a',\n",
       " '7c376650-5d91-4124-9057-9dfdd6984fbb',\n",
       " '1a76ae7a-7668-4def-b172-0d50484f2530',\n",
       " 'd4b4b132-d5ef-4c41-ad31-0f2cc5fb5b02',\n",
       " '79652c84-fdaf-445f-a9cd-95e4901e0eb4',\n",
       " '155a1e9c-2e52-4639-ad26-b462dc0315c4',\n",
       " '868229e8-0f95-4eb5-8db6-30e329654a0d']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'823c12cd-1c46-4136-96f3-d22841a447c9\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"disabled_db_uuids=['823c12cd-1c46-4136-96f3-d22841a447c9']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Error: The datablob has already been deleted.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Error: The datablob has already been deleted.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for datasource rm\n",
    "# Testing positive scenario with quite\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    with generate_db() as db:\n",
    "        db_uuid = db.uuid\n",
    "\n",
    "        # Deleting the created data source from the server\n",
    "        result = runner.invoke(app, [\"rm\", db_uuid, \"-q\"])\n",
    "        deleted_uuid = result.stdout[:-1]\n",
    "\n",
    "        display(deleted_uuid)\n",
    "\n",
    "        assert result.exit_code == 0\n",
    "        assert deleted_uuid == db_uuid\n",
    "\n",
    "        # List the existing data source ids in server and make sure the deleted id is not present in the server\n",
    "        format_str = \"{'datablob_uuid': '{}'}\"\n",
    "        ls_result = runner.invoke(app, [\"ls\", \"--format\", format_str])\n",
    "        ls_uuids = get_uuids_from_result(ls_result)\n",
    "\n",
    "        display(ls_uuids)\n",
    "        assert deleted_uuid not in ls_uuids\n",
    "\n",
    "        # ls with quiet and disabled = True\n",
    "        result = runner.invoke(app, [\"ls\", \"--disabled\", \"-q\"])\n",
    "\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 0\n",
    "\n",
    "        disabled_db_uuids = get_uuids_from_result(result)\n",
    "\n",
    "        display(f\"{disabled_db_uuids=}\")\n",
    "        assert deleted_uuid in disabled_db_uuids\n",
    "\n",
    "        # Testing negative scenario. Deleting already deleted data source\n",
    "        result = runner.invoke(app, [\"rm\", deleted_uuid, \"-q\"])\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 1\n",
    "\n",
    "        # Testing negative scenario. Getting the details of the deleted data source\n",
    "        result = runner.invoke(app, [\"details\", deleted_uuid])\n",
    "        display(result.stdout)\n",
    "        assert result.exit_code == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: The datablob uuid is incorrect. Please try again.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for datasource rm\n",
    "# Testing negative scenario. Deleting invalid data source\n",
    "with set_airt_service_token_envvar():\n",
    "    # Testing negative scenario. Deleting already deleted data source\n",
    "    result = runner.invoke(app, [\"rm\", RANDOM_UUID_FOR_TESTING, \"-q\"])\n",
    "\n",
    "    display(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command()\n",
    "@helper.display_formated_table\n",
    "@helper.requires_auth_token\n",
    "def tag(\n",
    "    uuid: str = typer.Option(\n",
    "        ...,\n",
    "        \"--datablob_uuid\",\n",
    "        \"-uuid\",\n",
    "        help=\"Datablob uuid in the server.\",\n",
    "    ),\n",
    "    name: str = typer.Option(\n",
    "        ...,\n",
    "        \"--name\",\n",
    "        \"-n\",\n",
    "        help=\"A string to tag the datablob.\",\n",
    "    ),\n",
    "    format: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--format\",\n",
    "        \"-f\",\n",
    "        help=\"Format output and show only the given column(s) values.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    ") -> Dict[\"str\", Union[pd.DataFrame, str]]:\n",
    "    \"\"\"Tag an existing datablob in the server.\"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob(uuid=uuid)\n",
    "    df = db.tag(name=name)\n",
    "\n",
    "    df[\"pulled_on\"] = helper.humanize_date(df[\"pulled_on\"])\n",
    "    df[\"folder_size\"] = helper.humanize_size(df[\"folder_size\"])\n",
    "\n",
    "    return {\"df\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: root tag [OPTIONS]\\n\\n  Tag an existing datablob in the server.\\n\\nOptions:\\n  -uuid, --datablob_uuid TEXT  Datablob uuid in the server.  [required]\\n  -n, --name TEXT              A string to tag the datablob.  [required]\\n  -f, --format TEXT            Format output and show only the given column(s)\\n                               values.\\n  -d, --debug                  Set logger level to DEBUG and output everything.\\n  --help                       Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"_db.uuid='f2e00205-6568-4249-8f0e-176142cc5ac2'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         tags\\nf2e00205-6568-4249-8f0e-176142cc5ac2  latest, v1.1.0\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests for tag\n",
    "# Testing positive scenario\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    with generate_db(force_create=True) as db:\n",
    "        db_uuid = db.uuid\n",
    "\n",
    "        # Tag the data source\n",
    "        format_str = \"{'datablob_uuid': '{}', 'tags': '{}'}\"\n",
    "        result = runner.invoke(\n",
    "            app, [\"tag\", \"-uuid\", db_uuid, \"-n\", \"v1.1.0\", \"--format\", format_str]\n",
    "        )\n",
    "\n",
    "        display(result.stdout)\n",
    "\n",
    "        assert result.exit_code == 0\n",
    "        assert \"v1.1.0\" in str(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command(\"from-local\")\n",
    "@helper.requires_auth_token\n",
    "def from_local(\n",
    "    path: str = typer.Option(\n",
    "        ...,\n",
    "        \"--path\",\n",
    "        \"-p\",\n",
    "        help=\"The relative or absolute path to a local CSV/parquet file or to a directory containing the CSV/parquet files.\",\n",
    "    ),\n",
    "    cloud_provider: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--cloud-provider\",\n",
    "        \"-cp\",\n",
    "        help=\"The destination cloud storage provider's name to store the datablob. Currently, the API only supports **aws** and **azure** as cloud storage providers. If **None** (default value), then **aws**  will be used as the cloud storage provider.\",\n",
    "    ),\n",
    "    region: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--region\",\n",
    "        \"-r\",\n",
    "        help=\"The destination cloud provider's region to save your datablob. If **None** (default value) then the default region will be assigned based on the cloud provider. \"\n",
    "        \"In the case of **aws**, **eu-west-1** will be used and in the case of **azure**, **westeurope** will be used. \"\n",
    "        \"The supported AWS regions are: ap-northeast-1, ap-northeast-2, ap-south-1, ap-southeast-1, ap-southeast-2, ca-central-1, eu-central-1, \"\n",
    "        \"eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2. The supported Azure Blob Storage \"\n",
    "        \"regions are: australiacentral, australiacentral2, australiaeast, australiasoutheast, brazilsouth, canadacentral, canadaeast, centralindia, \"\n",
    "        \"centralus, eastasia, eastus, eastus2, francecentral, francesouth, germanynorth, germanywestcentral, japaneast, japanwest, koreacentral, koreasouth, \"\n",
    "        \"northcentralus, northeurope, norwayeast, norwaywest, southafricanorth, southafricawest, southcentralus, southeastasia, southindia, switzerlandnorth, \"\n",
    "        \"switzerlandwest, uaecentral, uaenorth, uksouth, ukwest, westcentralus, westeurope, westindia, westus, westus2.\",\n",
    "    ),\n",
    "    tag: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--tag\",\n",
    "        \"-t\",\n",
    "        help=\"A string to tag the datablob. If not passed, then the tag **latest** will be assigned to the datablob.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output data id only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    ") -> None:\n",
    "    \"\"\"Create and return a datablob from local csv file.\n",
    "\n",
    "    The API currently allows users to create datablobs from CSV or Parquet files. We intend to support additional file formats in future releases.\n",
    "    \"\"\"\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    if quiet:\n",
    "        db = DataBlob.from_local(\n",
    "            path=path,\n",
    "            cloud_provider=cloud_provider,\n",
    "            region=region,\n",
    "            tag=tag,\n",
    "            show_progress=False,\n",
    "        )\n",
    "        typer.echo(f\"{db.uuid}\")\n",
    "    else:\n",
    "        db = DataBlob.from_local(\n",
    "            path=path, cloud_provider=cloud_provider, region=region, tag=tag\n",
    "        )\n",
    "        typer.echo(f\"Successfully pulled the datablob uuid: {db.uuid}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Usage: root from-local [OPTIONS]\\n\\n  Create and return a datablob from local csv file.\\n\\n  The API currently allows users to create datablobs from CSV or Parquet files.\\n  We intend to support additional file formats in future releases.\\n\\nOptions:\\n  -p, --path TEXT             The relative or absolute path to a local\\n                              CSV/parquet file or to a directory containing the\\n                              CSV/parquet files.  [required]\\n  -cp, --cloud-provider TEXT  The destination cloud storage provider's name to\\n                              store the datablob. Currently, the API only\\n                              supports **aws** and **azure** as cloud storage\\n                              providers. If **None** (default value), then\\n                              **aws**  will be used as the cloud storage\\n                              provider.\\n  -r, --region TEXT           The destination cloud provider's region to save\\n                              your datablob. If **None** (default value) then\\n                              the default region will be assigned based on the\\n                              cloud provider. In the case of **aws**, **eu-\\n                              west-1** will be used and in the case of\\n                              **azure**, **westeurope** will be used. The\\n                              supported AWS regions are: ap-northeast-1, ap-\\n                              northeast-2, ap-south-1, ap-southeast-1, ap-\\n                              southeast-2, ca-central-1, eu-central-1, eu-\\n                              north-1, eu-west-1, eu-west-2, eu-west-3, sa-\\n                              east-1, us-east-1, us-east-2, us-west-1, us-\\n                              west-2. The supported Azure Blob Storage regions\\n                              are: australiacentral, australiacentral2,\\n                              australiaeast, australiasoutheast, brazilsouth,\\n                              canadacentral, canadaeast, centralindia,\\n                              centralus, eastasia, eastus, eastus2,\\n                              francecentral, francesouth, germanynorth,\\n                              germanywestcentral, japaneast, japanwest,\\n                              koreacentral, koreasouth, northcentralus,\\n                              northeurope, norwayeast, norwaywest,\\n                              southafricanorth, southafricawest, southcentralus,\\n                              southeastasia, southindia, switzerlandnorth,\\n                              switzerlandwest, uaecentral, uaenorth, uksouth,\\n                              ukwest, westcentralus, westeurope, westindia,\\n                              westus, westus2.\\n  -t, --tag TEXT              A string to tag the datablob. If not passed, then\\n                              the tag **latest** will be assigned to the\\n                              datablob.\\n  -q, --quiet                 Output data id only.\\n  -d, --debug                 Set logger level to DEBUG and output everything.\\n  --help                      Show this message and exit.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"from-local\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/_common_metadata to ../../../tmp/test_s3_download_qoxtkeyn/parquet/_common_metadata\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.16.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.16.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/_metadata to ../../../tmp/test_s3_download_qoxtkeyn/parquet/_metadata\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.17.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.17.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.19.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.19.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.18.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.18.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.2.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.2.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.3.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.3.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.4.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.4.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.5.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.5.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.6.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.6.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.8.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.8.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.7.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.7.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.9.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.9.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.10.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.10.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.13.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.13.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.11.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.11.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.12.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.12.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.14.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.14.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.15.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.15.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.0.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.0.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.1.parquet to ../../../tmp/test_s3_download_qoxtkeyn/parquet/part.1.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/test_s3_download_qoxtkeyn/csv/file-1.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-15.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-6.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-8.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-17.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-19.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-18.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-5.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-14.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-0.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-10.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-7.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-12.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-4.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-2.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-3.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-16.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-13.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-9.csv'),\n",
       " Path('/tmp/test_s3_download_qoxtkeyn/csv/file-11.csv')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\r  0%|          | 0/20 [00:00<?, ?it/s]\\r  5%|▌         | 1/20 [00:00<00:10,  1.85it/s]\\r 10%|█         | 2/20 [00:01<00:10,  1.76it/s]\\r 15%|█▌        | 3/20 [00:01<00:09,  1.77it/s]\\r 20%|██        | 4/20 [00:02<00:08,  1.83it/s]\\r 25%|██▌       | 5/20 [00:02<00:08,  1.73it/s]\\r 30%|███       | 6/20 [00:03<00:07,  1.76it/s]\\r 35%|███▌      | 7/20 [00:03<00:07,  1.72it/s]\\r 40%|████      | 8/20 [00:04<00:06,  1.78it/s]\\r 45%|████▌     | 9/20 [00:05<00:06,  1.78it/s]\\r 50%|█████     | 10/20 [00:05<00:05,  1.80it/s]\\r 55%|█████▌    | 11/20 [00:06<00:04,  1.85it/s]\\r 60%|██████    | 12/20 [00:06<00:04,  1.80it/s]\\r 65%|██████▌   | 13/20 [00:07<00:04,  1.57it/s]\\r 70%|███████   | 14/20 [00:08<00:03,  1.64it/s]\\r 75%|███████▌  | 15/20 [00:09<00:03,  1.39it/s]\\r 80%|████████  | 16/20 [00:09<00:03,  1.32it/s]\\r 85%|████████▌ | 17/20 [00:10<00:02,  1.28it/s]\\r 90%|█████████ | 18/20 [00:11<00:01,  1.29it/s]\\r 95%|█████████▌| 19/20 [00:12<00:00,  1.25it/s]\\r100%|██████████| 20/20 [00:13<00:00,  1.22it/s]\\r100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\\nSuccessfully pulled the datablob uuid: fa68cd32-b04f-4550-a196-e9f5a3c44d0f.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'e0015859-78a7-42dd-bd86-7db9f7f121a4'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type    source                                    region     cloud_provider    tags    pulled_on    folder_size    user_uuid                             error    disabled    ready\\ne0015859-78a7-42dd-bd86-7db9f7f121a4  <none>              local   local:/tmp/test_s3_download_qoxtkeyn/csv  eu-west-1  aws               latest  None         unknown        4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       False\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'temp_dir.exists()=False'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | include: false\n",
    "\n",
    "# Helper function to download a sample csv file into the temp directory for testing Datablob local csv command\n",
    "\n",
    "\n",
    "def get_test_csv_path() -> Path:\n",
    "    \"\"\"Downloads the account_312571_events from the s3 bucket and stores it in temp folder.\n",
    "    Finally converts the downloaded account_312571_events files to a csv file and returns the\n",
    "    path of the temp folder and the temp csv file.\n",
    "    \"\"\"\n",
    "    temp_dirpath = Path(tempfile.mkdtemp(prefix=\"test_s3_download_\"))\n",
    "\n",
    "    !aws s3 sync {TEST_S3_URI} {temp_dirpath / \"parquet\"}\n",
    "\n",
    "    parquet_path = Path(temp_dirpath / \"parquet\")\n",
    "    csv_dirpath = Path(temp_dirpath / \"csv\")\n",
    "    os.mkdir(csv_dirpath)\n",
    "\n",
    "    for i, f in enumerate(list(parquet_path.glob(\"*.parquet\"))):\n",
    "        df = pd.read_parquet(f)\n",
    "        df.to_csv(csv_dirpath / f\"file-{i}.csv\", index=False)\n",
    "\n",
    "    display(list(csv_dirpath.glob(\"*\")))\n",
    "\n",
    "    return temp_dirpath, csv_dirpath\n",
    "\n",
    "\n",
    "# Testing multiple files upload.\n",
    "\n",
    "# Create temp directory\n",
    "temp_dir, csv_dirpath = get_test_csv_path()\n",
    "\n",
    "# Creating a new datasource\n",
    "cmd = [\"from-local\", \"--path\", f\"{csv_dirpath}\"]\n",
    "\n",
    "cmd_q = [\"from-local\", \"--path\", f\"{csv_dirpath}\", \"-q\"]\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    # Without quiet\n",
    "    result = runner.invoke(app, cmd)\n",
    "\n",
    "    display(result.stdout)\n",
    "    assert \"Successfully pulled the datablob uuid:\" in str(result.stdout)\n",
    "\n",
    "    # With quiet\n",
    "    result = runner.invoke(app, cmd_q)\n",
    "    db_uuid = result.stdout[:-1]\n",
    "    display(db_uuid)\n",
    "    assert len(remove_hypens_from_id(db_uuid)) == 32\n",
    "\n",
    "    result = runner.invoke(app, [\"details\", db_uuid])\n",
    "    display(result.stdout)\n",
    "    assert result.exit_code == 0\n",
    "    assert \"eu-west-1\" in result.stdout\n",
    "\n",
    "# Deleting the temp directory\n",
    "shutil.rmtree(temp_dir)\n",
    "display(f\"{temp_dir.exists()=}\")\n",
    "assert not temp_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/_metadata to ../../../tmp/test_s3_download_58lkcuny/parquet/_metadata\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/_common_metadata to ../../../tmp/test_s3_download_58lkcuny/parquet/_common_metadata\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.16.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.16.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.12.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.12.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.1.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.1.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.11.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.11.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.18.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.18.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.14.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.14.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.13.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.13.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.15.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.15.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.0.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.0.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.19.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.19.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.17.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.17.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.10.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.10.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.4.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.4.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.5.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.5.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.2.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.2.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.7.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.7.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.6.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.6.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.3.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.3.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.8.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.8.parquet\n",
      "download: s3://test-airt-service/ecommerce_behavior_notebooks/part.9.parquet to ../../../tmp/test_s3_download_58lkcuny/parquet/part.9.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/test_s3_download_58lkcuny/csv/file-1.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-15.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-6.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-8.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-17.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-19.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-18.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-5.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-14.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-0.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-10.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-7.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-12.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-4.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-2.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-3.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-16.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-13.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-9.csv'),\n",
       " Path('/tmp/test_s3_download_58lkcuny/csv/file-11.csv')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\\r100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\\nSuccessfully pulled the datablob uuid: 8feeb4d7-3343-4f1e-a25e-8f39822ee440.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bdfe2e41-a9da-4d68-9459-a03418eb7e0f\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'temp_dir.exists()=False'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing single files upload.\n",
    "\n",
    "# Create temp directory\n",
    "temp_dir, csv_dirpath = get_test_csv_path()\n",
    "\n",
    "# Creating a new datasource\n",
    "cmd = [\"from-local\", \"--path\", str(csv_dirpath / \"file-1.csv\")]\n",
    "\n",
    "cmd_q = [\"from-local\", \"--path\", str(csv_dirpath / \"file-1.csv\"), \"-q\"]\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    # Without quiet\n",
    "    result = runner.invoke(app, cmd)\n",
    "\n",
    "    display(result.stdout)\n",
    "    assert \"Successfully pulled the datablob uuid:\" in str(result.stdout)\n",
    "\n",
    "    # With quiet\n",
    "    result = runner.invoke(app, cmd_q)\n",
    "\n",
    "    display(result.stdout)\n",
    "    assert len(remove_hypens_from_id(result.stdout[:-1])) == 32\n",
    "\n",
    "\n",
    "# Deleting the temp directory\n",
    "shutil.rmtree(temp_dir)\n",
    "display(f\"{temp_dir.exists()=}\")\n",
    "assert not temp_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@app.command(\"from-clickhouse\")\n",
    "@helper.requires_auth_token\n",
    "def from_clickhouse(\n",
    "    host: str = typer.Option(..., help=\"Remote database host name.\"),\n",
    "    database: str = typer.Option(..., help=\"Database name.\"),\n",
    "    table: str = typer.Option(..., help=\"Table name.\"),\n",
    "    protocol: str = typer.Option(\n",
    "        ..., help='Protocol to use. The valid values are \"native\" and \"http\".'\n",
    "    ),\n",
    "    index_column: str = typer.Option(\n",
    "        ..., help=\"The column to use as index (row labels).\"\n",
    "    ),\n",
    "    timestamp_column: str = typer.Option(\n",
    "        ..., help=\"Timestamp column name in the tabel.\"\n",
    "    ),\n",
    "    port: int = typer.Option(\n",
    "        0,\n",
    "        help=\"Host port number. If not passed, then the default value **0** will be used.\",\n",
    "    ),\n",
    "    cloud_provider: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--cloud-provider\",\n",
    "        \"-cp\",\n",
    "        help=\"The destination cloud storage provider's name to store the datablob. Currently, the API only supports **aws** and **azure** as cloud storage providers. If **None** (default value), then **aws**  will be used as the cloud storage provider.\",\n",
    "    ),\n",
    "    region: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--region\",\n",
    "        \"-r\",\n",
    "        help=\"The destination cloud provider's region to save your datablob. If **None** (default value) then the default region will be assigned based on the cloud provider. \"\n",
    "        \"In the case of **aws**, **eu-west-1** will be used and in the case of **azure**, **westeurope** will be used. \"\n",
    "        \"The supported AWS regions are: ap-northeast-1, ap-northeast-2, ap-south-1, ap-southeast-1, ap-southeast-2, ca-central-1, eu-central-1, \"\n",
    "        \"eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2. The supported Azure Blob Storage \"\n",
    "        \"regions are: australiacentral, australiacentral2, australiaeast, australiasoutheast, brazilsouth, canadacentral, canadaeast, centralindia, \"\n",
    "        \"centralus, eastasia, eastus, eastus2, francecentral, francesouth, germanynorth, germanywestcentral, japaneast, japanwest, koreacentral, koreasouth, \"\n",
    "        \"northcentralus, northeurope, norwayeast, norwaywest, southafricanorth, southafricawest, southcentralus, southeastasia, southindia, switzerlandnorth, \"\n",
    "        \"switzerlandwest, uaecentral, uaenorth, uksouth, ukwest, westcentralus, westeurope, westindia, westus, westus2.\",\n",
    "    ),\n",
    "    username: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--username\",\n",
    "        \"-u\",\n",
    "        help=\"Database username. If not passed, the default value 'root' will be used unless the value is explicitly set in the environment variable **CLICKHOUSE_USERNAME**.\",\n",
    "    ),\n",
    "    password: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--password\",\n",
    "        \"-p\",\n",
    "        help=\"Database password. If not passed, the default value '' will be used unless the value is explicitly set in the environment variable **CLICKHOUSE_PASSWORD**.\",\n",
    "    ),\n",
    "    filters_json: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--filters-json\",\n",
    "        \"-f\",\n",
    "        help=\"Additional parameters to be used when importing data. For example, if you want to filter and extract data only for a specific user_id, pass '{\"\n",
    "        '\"user_id\"'\n",
    "        \": 1}'.\",\n",
    "    ),\n",
    "    tag: Optional[str] = typer.Option(\n",
    "        None,\n",
    "        \"--tag\",\n",
    "        \"-t\",\n",
    "        help=\"A string to tag the datablob. If not passed, then the tag **latest** will be assigned to the datablob.\",\n",
    "    ),\n",
    "    quiet: bool = typer.Option(\n",
    "        False,\n",
    "        \"--quiet\",\n",
    "        \"-q\",\n",
    "        help=\"Output datablob uuid only.\",\n",
    "    ),\n",
    "    debug: bool = typer.Option(\n",
    "        False,\n",
    "        \"--debug\",\n",
    "        \"-d\",\n",
    "        help=\"Set logger level to DEBUG and output everything.\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"Create and return a datablob that encapsulates the data from a ClickHouse database.\n",
    "\n",
    "    If the database requires authentication, pass the username/password as commandline arguments or store it in\n",
    "    the **CLICKHOUSE_USERNAME** and **CLICKHOUSE_PASSWORD** environment variables.\n",
    "    \"\"\"\n",
    "\n",
    "    filters = json.loads(filters_json) if filters_json else None\n",
    "\n",
    "    from airt.client import DataBlob\n",
    "\n",
    "    db = DataBlob.from_clickhouse(\n",
    "        host=host,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "        index_column=index_column,\n",
    "        timestamp_column=timestamp_column,\n",
    "        port=port,\n",
    "        username=username,\n",
    "        password=password,\n",
    "        filters=filters,\n",
    "        cloud_provider=cloud_provider,\n",
    "        region=region,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "    if quiet:\n",
    "        db.wait()\n",
    "        typer.echo(f\"{db.uuid}\")\n",
    "    else:\n",
    "        typer.echo(f\"Pulling datablob uuid: {db.uuid}\")\n",
    "        db.progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usage: root from-clickhouse [OPTIONS]\\n\\n  Create and return a datablob that encapsulates the data from a ClickHouse\\n  database.\\n\\n  If the database requires authentication, pass the username/password as\\n  commandline arguments or store it in the **CLICKHOUSE_USERNAME** and\\n  **CLICKHOUSE_PASSWORD** environment variables.\\n\\nOptions:\\n  --host TEXT                 Remote database host name.  [required]\\n  --database TEXT             Database name.  [required]\\n  --table TEXT                Table name.  [required]\\n  --protocol TEXT             Protocol to use. The valid values are \"native\" and\\n                              \"http\".  [required]\\n  --index-column TEXT         The column to use as index (row labels).\\n                              [required]\\n  --timestamp-column TEXT     Timestamp column name in the tabel.  [required]\\n  --port INTEGER              Host port number. If not passed, then the default\\n                              value **0** will be used.  [default: 0]\\n  -cp, --cloud-provider TEXT  The destination cloud storage provider\\'s name to\\n                              store the datablob. Currently, the API only\\n                              supports **aws** and **azure** as cloud storage\\n                              providers. If **None** (default value), then\\n                              **aws**  will be used as the cloud storage\\n                              provider.\\n  -r, --region TEXT           The destination cloud provider\\'s region to save\\n                              your datablob. If **None** (default value) then\\n                              the default region will be assigned based on the\\n                              cloud provider. In the case of **aws**, **eu-\\n                              west-1** will be used and in the case of\\n                              **azure**, **westeurope** will be used. The\\n                              supported AWS regions are: ap-northeast-1, ap-\\n                              northeast-2, ap-south-1, ap-southeast-1, ap-\\n                              southeast-2, ca-central-1, eu-central-1, eu-\\n                              north-1, eu-west-1, eu-west-2, eu-west-3, sa-\\n                              east-1, us-east-1, us-east-2, us-west-1, us-\\n                              west-2. The supported Azure Blob Storage regions\\n                              are: australiacentral, australiacentral2,\\n                              australiaeast, australiasoutheast, brazilsouth,\\n                              canadacentral, canadaeast, centralindia,\\n                              centralus, eastasia, eastus, eastus2,\\n                              francecentral, francesouth, germanynorth,\\n                              germanywestcentral, japaneast, japanwest,\\n                              koreacentral, koreasouth, northcentralus,\\n                              northeurope, norwayeast, norwaywest,\\n                              southafricanorth, southafricawest, southcentralus,\\n                              southeastasia, southindia, switzerlandnorth,\\n                              switzerlandwest, uaecentral, uaenorth, uksouth,\\n                              ukwest, westcentralus, westeurope, westindia,\\n                              westus, westus2.\\n  -u, --username TEXT         Database username. If not passed, the default\\n                              value \\'root\\' will be used unless the value is\\n                              explicitly set in the environment variable\\n                              **CLICKHOUSE_USERNAME**.\\n  -p, --password TEXT         Database password. If not passed, the default\\n                              value \\'\\' will be used unless the value is\\n                              explicitly set in the environment variable\\n                              **CLICKHOUSE_PASSWORD**.\\n  -f, --filters-json TEXT     Additional parameters to be used when importing\\n                              data. For example, if you want to filter and\\n                              extract data only for a specific user_id, pass\\n                              \\'{\"user_id\": 1}\\'.\\n  -t, --tag TEXT              A string to tag the datablob. If not passed, then\\n                              the tag **latest** will be assigned to the\\n                              datablob.\\n  -q, --quiet                 Output datablob uuid only.\\n  -d, --debug                 Set logger level to DEBUG and output everything.\\n  --help                      Show this message and exit.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert_has_help([\"from-clickhouse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pulling datablob uuid: a17b8b8f-74a7-4d8e-9f53-803c14ad726f\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r  0%|          | 0/1 [00:10<?, ?it/s]\\r  0%|          | 0/1 [00:15<?, ?it/s]\\r  0%|          | 0/1 [00:20<?, ?it/s]\\r100%|██████████| 1/1 [00:25<00:00,  5.06s/it]\\r100%|██████████| 1/1 [00:25<00:00, 25.40s/it]\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'b3826d4f-852b-4676-af05-933316c4a56a'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type    source                                                       region     cloud_provider    tags    pulled_on      folder_size    user_uuid                             error    disabled    ready\\nb3826d4f-852b-4676-af05-933316c4a56a  <none>              db      clickhouse+native://20.73.247.24:0/infobip/airt_training_3m  eu-west-1  aws               latest  3 seconds ago  8.9 MB         4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       True\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'247e6345-8940-4d75-8845-576d2b652955'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'datablob_uuid                         datasource_uuids    type    source                                                       region     cloud_provider    tags    pulled_on      folder_size    user_uuid                             error    disabled    ready\\n247e6345-8940-4d75-8845-576d2b652955  <none>              db      clickhouse+native://20.73.247.24:0/infobip/airt_training_3m  eu-west-3  aws               latest  4 seconds ago  8.9 MB         4b5131a3-6562-413d-abf2-1103275bf945  <none>   False       True\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests for from-clickhouse. Testing positive scenario.\n",
    "\n",
    "cmd = [\n",
    "    \"from-clickhouse\",\n",
    "    \"--host\",\n",
    "    os.environ.get(\"CLICKHOUSE_HOST\"),\n",
    "    \"--database\",\n",
    "    os.environ.get(\"CLICKHOUSE_DATABASE\"),\n",
    "    \"--table\",\n",
    "    os.environ.get(\"CLICKHOUSE_EVENTS_TABLE\"),\n",
    "    \"--protocol\",\n",
    "    \"native\",\n",
    "    \"--index-column\",\n",
    "    \"PersonId\",\n",
    "    \"--timestamp-column\",\n",
    "    \"OccurredTimeTicks\",\n",
    "    \"--filters-json\",\n",
    "    '{\"AccountId\": 312571}',\n",
    "]\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    # Without quiet (verbose)\n",
    "    result = runner.invoke(app, cmd)\n",
    "    display(result.stdout)\n",
    "    assert \"Pulling datablob uuid: \" in result.stdout\n",
    "\n",
    "    result = runner.invoke(app, cmd + [\"-q\"])\n",
    "    db_uuid = result.stdout[:-1]\n",
    "    display(db_uuid)\n",
    "    assert len(remove_hypens_from_id(db_uuid)) == 32\n",
    "\n",
    "    result = runner.invoke(app, [\"details\", db_uuid])\n",
    "    display(result.stdout)\n",
    "    assert result.exit_code == 0\n",
    "    assert \"eu-west-1\" in result.stdout\n",
    "\n",
    "    result = runner.invoke(app, cmd + [\"-cp\", \"aws\", \"--region\", \"eu-west-3\", \"-q\"])\n",
    "    db_uuid = result.stdout[:-1]\n",
    "    display(db_uuid)\n",
    "    assert len(remove_hypens_from_id(db_uuid)) == 32\n",
    "\n",
    "    result = runner.invoke(app, [\"details\", db_uuid])\n",
    "    display(result.stdout)\n",
    "    assert result.exit_code == 0\n",
    "    assert \"eu-west-3\" in result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pulling datablob uuid: b957e2c5-4527-41d8-90b2-46e0c1bada8f\\n\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:00<?, ?it/s]\\r  0%|          | 0/1 [00:05<?, ?it/s]\\r  0%|          | 0/1 [00:10<?, ?it/s]\\nError: Orig exception: Code: 81.\\nDB::Exception: Database `fake-database` doesn't exist. Stack trace:\\n\\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0xddb0df5 in /usr/bin/clickhouse\\n1. ? @ 0x883ade4 in /usr/bin/clickhouse\\n2. DB::Databas\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tests for from-clickhouse. Testing negative scenario.\n",
    "\n",
    "cmd = [\n",
    "    \"from-clickhouse\",\n",
    "    \"--host\",\n",
    "    os.environ.get(\"CLICKHOUSE_HOST\"),\n",
    "    \"--database\",\n",
    "    \"fake-database\",\n",
    "    \"--table\",\n",
    "    \"fake-table\",\n",
    "    \"--protocol\",\n",
    "    \"native\",\n",
    "    \"--index-column\",\n",
    "    \"PersonId\",\n",
    "    \"--timestamp-column\",\n",
    "    \"OccurredTimeTicks\",\n",
    "    \"-f\",\n",
    "    '{\"AccountId\": 312571}',\n",
    "]\n",
    "\n",
    "\n",
    "with set_airt_service_token_envvar():\n",
    "    # Without quiet (verbose)\n",
    "    result = runner.invoke(app, cmd)\n",
    "    display(result.stdout)\n",
    "    assert result.exit_code == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
